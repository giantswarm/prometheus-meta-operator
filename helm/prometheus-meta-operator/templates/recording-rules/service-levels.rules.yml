apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    {{- include "labels.common" . | nindent 4 }}
  name: service-level
  namespace: '{{ include "resource.default.namespace" . }}'
spec:
  groups:
  - name: service-level
    rules:

    # -- standard burnrates
    - expr: "vector(36)"
      record: slo_burnrate_high
    - expr: "vector(12)"
      record: slo_burnrate_low

      # -- api-server
    - expr: "count(up{app='kubernetes'}) by (cluster_type,cluster_id)"
      labels:
        class: HIGH
        service: api-server
      record: raw_slo_requests
    - expr: "sum((up{app='kubernetes'} * -1) + 1) by (cluster_type,cluster_id)"
      labels:
        class: HIGH
        service: api-server
      record: raw_slo_errors
    - expr: "vector(0.001)"
      labels:
        service: api-server
      record: slo_target

      # -- kubelet
    - expr: "kube_node_status_condition{condition='Ready'}"
      labels:
        class: MEDIUM
        service: kubelet
      record: raw_slo_requests
    - expr: |
        (
          kube_node_status_condition{condition="Ready", status!="true"}
          and
          on (node) kube_node_spec_unschedulable == 0
        )
        and
        on (node) time() - kube_node_created > 10 * 60
      labels:
        class: MEDIUM
        service: kubelet
      record: raw_slo_errors
    - expr: "vector(0.01)"
      labels:
        service: kubelet
      record: slo_target

      # -- jose (a fake erroring service)
    - expr: "vector(100)"
      labels:
        class: MEDIUM
        service: jose
        cluster_id: gremlin
        cluster_type: management_cluster
      record: raw_slo_requests
    - expr: "vector(1)"
      labels:
        class: MEDIUM
        service: jose
        cluster_id: gremlin
        cluster_type: management_cluster
      record: raw_slo_errors
    - expr: "vector(0.01)"
      labels:
        service: jose
      record: slo_target

    # -- managed-apps
    # record when pods of a daemonset with label "label_giantswarm_io_monitoring_basic_sli" are down
    - expr: |
        label_replace(kube_daemonset_status_number_unavailable
        and on(daemonset,cluster_id,cluster_type,namespace, workload_name)
        kube_daemonset_labels{label_giantswarm_io_monitoring_basic_sli='true'}, "service", "$1", "workload_name", "(.*)" )
      labels:
        class: MEDIUM
        area: managed-apps
      record: raw_slo_errors
    # record when pods of a deployment with label "label_giantswarm_io_monitoring_basic_sli" are down
    - expr: |
        label_replace(kube_deployment_status_replicas_unavailable
        and on(deployment,cluster_id,cluster_type,namespace, workload_name) 
        kube_deployment_labels{label_giantswarm_io_monitoring_basic_sli='true'}, "service", "$1", "workload_name", "(.*)" )
      labels:
        class: MEDIUM
        area: managed-apps
      record: raw_slo_errors
    # record when pods of a statefulset with label "label_giantswarm_io_monitoring_basic_sli" are down
    - expr: |
        label_replace(kube_statefulset_status_replicas - kube_statefulset_status_replicas_current
        and on(statefulset,cluster_id,cluster_type,namespace, workload_name) 
        kube_statefulset_labels{label_giantswarm_io_monitoring_basic_sli='true'}, "service", "$1", "workload_name", "(.*)" )
      labels:
        class: MEDIUM
        area: managed-apps
      record: raw_slo_errors
    - expr: |
        label_replace(kube_daemonset_status_desired_number_scheduled
        and on(daemonset,cluster_id,cluster_type,namespace, workload_name)
        kube_daemonset_labels{label_giantswarm_io_monitoring_basic_sli='true'}, "service", "$1", "workload_name", "(.*)" )
      labels:
        class: MEDIUM
        area: managed-apps
      record: raw_slo_requests
    - expr: |
        label_replace(kube_deployment_status_replicas
        and on(deployment,cluster_id,cluster_type,namespace, workload_name) 
        kube_deployment_labels{label_giantswarm_io_monitoring_basic_sli='true'}, "service", "$1", "workload_name", "(.*)" )
      labels:
        class: MEDIUM
        area: managed-apps
      record: raw_slo_requests
    - expr: |
        label_replace(kube_statefulset_status_replicas
        and on(statefulset,cluster_id,cluster_type,namespace, workload_name) 
        kube_statefulset_labels{label_giantswarm_io_monitoring_basic_sli='true'}, "service", "$1", "workload_name", "(.*)" )
      labels:
        class: MEDIUM
        area: managed-apps
      record: raw_slo_requests
    - expr: sum by (service) (raw_slo_errors{area="managed-apps"} - raw_slo_errors{area="managed-apps"}) + 0.05
      record: slo_target

      # -- generic stuff
    - expr: sum(raw_slo_requests) by (service, cluster_type, cluster_id)
      record: slo_requests
    - expr: sum(raw_slo_errors) by (service, cluster_type, cluster_id)
      record: slo_errors
    - expr: sum(sum_over_time(raw_slo_errors[5m])) by (cluster_type, cluster_id, service, class) / sum(sum_over_time(raw_slo_requests[5m])) by (cluster_type, cluster_id, service, class)
      record: slo_errors_per_request:ratio_rate5m
    - expr: sum(sum_over_time(raw_slo_errors[30m])) by (cluster_type, cluster_id, service, class) / sum(sum_over_time(raw_slo_requests[30m])) by (cluster_type, cluster_id, service, class)
      record: slo_errors_per_request:ratio_rate30m
    - expr: sum(sum_over_time(raw_slo_errors[1h])) by (cluster_type, cluster_id, service, class) / sum(sum_over_time(raw_slo_requests[1h])) by (cluster_type, cluster_id, service, class)
      record: slo_errors_per_request:ratio_rate1h
    - expr: sum(sum_over_time(raw_slo_errors[6h])) by (cluster_type, cluster_id, service, class) / sum(sum_over_time(raw_slo_requests[6h])) by (cluster_type, cluster_id, service, class)
      record: slo_errors_per_request:ratio_rate6h
    - expr: slo_target * slo_target*scalar(min(slo_burnrate_high))
      record: slo_target_high
    - expr: slo_target * slo_target*scalar(min(slo_burnrate_low))
      record: slo_target_low
