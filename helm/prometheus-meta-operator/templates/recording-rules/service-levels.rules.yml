apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    {{- include "labels.common" . | nindent 4 }}
  name: service-level
  namespace: '{{ include "resource.default.namespace" . }}'
spec:
  groups:
  - name: service-level
    rules:
      # -- api-server
    - expr: "count(up{app='kubernetes'}) by (cluster_type,cluster_id)"
      labels:
        class: HIGH
        area: kaas
        service: api-server
      record: raw_slo_requests
    # The first statement ensures that an api-server error is counted if the kubernetes api is not up for a specific cluster.
    # The next statement returns 1 for a cluster with "updated", "created" or unknown (absent) status.
    # It returns 0 for clusters in "updating", "creating" and "deleting" status. 
    # By multiplying with this statement we ensure that errors for transitioning clusters are not counted.
    - expr: sum((up{app='kubernetes'} * -1) + 1) by (cluster_id, cluster_type) * 
            ignoring (cluster_type) group_left (cluster_id) 
            (
              max(cluster_operator_cluster_status{status=~"Updated|Created"}) by (cluster_id) 
              or absent(cluster_operator_cluster_status)
            )
      labels:
        class: HIGH
        area: kaas
        service: api-server
      record: raw_slo_errors
      # -- 99,9% availability
    - expr: "vector((1 - 0.999))"
      labels:
        area: kaas
        service: api-server
      record: slo_target

      # -- kubelet
    - expr: "kube_node_status_condition{condition='Ready'}"
      labels:
        class: MEDIUM
        area: kaas
        service: kubelet
      record: raw_slo_requests
    - expr: |
        (
          kube_node_status_condition{condition="Ready", status!="true"}
          and
          on (node) kube_node_spec_unschedulable == 0
        )
        and
        on (node) time() - kube_node_created > 10 * 60
      labels:
        area: kaas
        class: MEDIUM
        service: kubelet
      record: raw_slo_errors
      # -- 99% availability
    - expr: "vector((1 - 0.99))"
      labels:
        area: kaas
        service: kubelet
      record: slo_target

      # -- node-exporter
      # record of number of node-exporters.
    - expr: count(up{app="node-exporter", role!="bastion"}) by (cluster_type, cluster_id)
      labels:
        class: MEDIUM
        area: kaas
        service: node-exporter
      record: raw_slo_requests
      # record of number of node-exporters that are down.
      # up == 1 when node-exporters are up, and up == 0 when node-exporters are down -
      # to get a count of the number of node-exporters that are down,
      # multiply by -1 to get -1 for node-exporters that are up, and 0 for node-exporters that are down,
      # then add 1 to get 0 for node-exporters that are up, and 1 for node-exporters that are down,
      # then sum.
    - expr: sum((up{app='node-exporter', role!="bastion"} * -1) + 1) by (cluster_type, cluster_id)
      labels:
        area: kaas
        class: MEDIUM
        service: node-exporter
      record: raw_slo_errors
      # -- 99% availability
    - expr: "vector((1 - 0.99))"
      labels:
        area: kaas
        service: node-exporter
      record: slo_target

    # -- managed-apps
    # -- error recording rules
    # record when pods of a daemonset with label "label_giantswarm_io_monitoring_basic_sli" are down
    - expr: |
        label_replace(
          kube_daemonset_status_number_unavailable
          and on(daemonset,cluster_id,cluster_type,namespace,workload_name)
          kube_daemonset_labels{label_giantswarm_io_monitoring_basic_sli='true'},
        "service", "$1", "workload_name", "(.*)" )
      labels:
        class: MEDIUM
        area: managed-apps
      record: raw_slo_errors
    # record when pods of a deployment with label "label_giantswarm_io_monitoring_basic_sli" are down
    - expr: |
        label_replace(
          kube_deployment_status_replicas_unavailable
          and on(deployment,cluster_id,cluster_type,namespace, workload_name)
          kube_deployment_labels{label_giantswarm_io_monitoring_basic_sli='true'},
        "service", "$1", "workload_name", "(.*)" )
      labels:
        class: MEDIUM
        area: managed-apps
      record: raw_slo_errors
    # record when pods of a statefulset with label "label_giantswarm_io_monitoring_basic_sli" are down
    - expr: |
        label_replace(
          kube_statefulset_status_replicas - kube_statefulset_status_replicas_current
          and on(statefulset,cluster_id,cluster_type,namespace, workload_name)
          kube_statefulset_labels{label_giantswarm_io_monitoring_basic_sli='true'},
        "service", "$1", "workload_name", "(.*)" )
      labels:
        class: MEDIUM
        area: managed-apps
      record: raw_slo_errors
    # -- present pods recording rules
    - expr: |
        label_replace(
          kube_daemonset_status_desired_number_scheduled
          and on(daemonset,cluster_id,cluster_type,namespace, workload_name)
          kube_daemonset_labels{label_giantswarm_io_monitoring_basic_sli='true'},
        "service", "$1", "workload_name", "(.*)" )
      labels:
        class: MEDIUM
        area: managed-apps
      record: raw_slo_requests
    - expr: |
        label_replace(
          kube_deployment_status_replicas
          and on(deployment,cluster_id,cluster_type,namespace, workload_name)
          kube_deployment_labels{label_giantswarm_io_monitoring_basic_sli='true'},
        "service", "$1", "workload_name", "(.*)" )
      labels:
        class: MEDIUM
        area: managed-apps
      record: raw_slo_requests
    - expr: |
        label_replace(
          kube_statefulset_status_replicas
          and on(statefulset,cluster_id,cluster_type,namespace, workload_name)
          kube_statefulset_labels{label_giantswarm_io_monitoring_basic_sli='true'},
        "service", "$1", "workload_name", "(.*)" )
      labels:
        class: MEDIUM
        area: managed-apps
      record: raw_slo_requests
      # -- 99% availability
      # -- this expression collects all the services from the area managed apps and assigns the same slo target to all of them
    - expr: sum by (service, area) (raw_slo_errors{area="managed-apps"} - raw_slo_errors{area="managed-apps"}) + 1-0.99
      record: slo_target

      # -- generic stuff
      # -- standard burnrates based on https://sre.google/workbook/alerting-on-slos/#6-multiwindow-multi-burn-rate-alerts
    - expr: "vector(36)"
      record: slo_burnrate_high
    - expr: "vector(12)"
      record: slo_burnrate_low
    - expr: sum(raw_slo_requests) by (service, cluster_type, cluster_id, area)
      record: slo_requests
    - expr: sum(raw_slo_errors) by (service, cluster_type, cluster_id, area)
      record: slo_errors
    - expr: sum(sum_over_time(raw_slo_errors[5m])) by (cluster_type, cluster_id, service, class, area) / sum(sum_over_time(raw_slo_requests[5m])) by (cluster_type, cluster_id, service, class, area)
      record: slo_errors_per_request:ratio_rate5m
    - expr: sum(sum_over_time(raw_slo_errors[30m])) by (cluster_type, cluster_id, service, class, area) / sum(sum_over_time(raw_slo_requests[30m])) by (cluster_type, cluster_id, service, class, area)
      record: slo_errors_per_request:ratio_rate30m
    - expr: sum(sum_over_time(raw_slo_errors[1h])) by (cluster_type, cluster_id, service, class, area) / sum(sum_over_time(raw_slo_requests[1h])) by (cluster_type, cluster_id, service, class, area)
      record: slo_errors_per_request:ratio_rate1h
    - expr: sum(sum_over_time(raw_slo_errors[6h])) by (cluster_type, cluster_id, service, class, area) / sum(sum_over_time(raw_slo_requests[6h])) by (cluster_type, cluster_id, service, class, area)
      record: slo_errors_per_request:ratio_rate6h
    - expr: sum(sum_over_time(raw_slo_errors[2h])) by (cluster_type, cluster_id, service, class, area) / sum(sum_over_time(raw_slo_requests[2h])) by (cluster_type, cluster_id, service, class, area)
      record: slo_errors_per_request:ratio_rate2h
    - expr: sum(sum_over_time(raw_slo_errors[24h])) by (cluster_type, cluster_id, service, class, area) / sum(sum_over_time(raw_slo_requests[24h])) by (cluster_type, cluster_id, service, class, area)
      record: slo_errors_per_request:ratio_rate24h
    - expr: sum(sum_over_time(raw_slo_errors[3d])) by (cluster_type, cluster_id, service, class, area) / sum(sum_over_time(raw_slo_requests[3d])) by (cluster_type, cluster_id, service, class, area)
      record: slo_errors_per_request:ratio_rate3d
      # -- We use the `min` function here because we have `slo_burnrate_high` for each installation and cluster_id, even though the metric value is always the same.
    - expr: slo_target*scalar(min(slo_burnrate_high))
      record: slo_threshold_high
    - expr: slo_target*scalar(min(slo_burnrate_low))
      record: slo_threshold_low
